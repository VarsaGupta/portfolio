[{"content":"\rIntroduction I\u0026rsquo;ve developed an LLM(GEN AI) PACK which is deployed on CELLSTRAT Hub. This pack contains a series of projects developed as part of the LangChain initiative, demonstrating various applications and use cases leveraging language models and generative AI. Each project is designed to showcase the capabilities of different technologies and platforms.\nTable Of Contents Here\u0026rsquo;s a brief description for each item in the table of contents:\nLLM1-Basics of LLM: This section covers the fundamentals of LLM (Large Language Models), providing an introduction to its concepts, applications, and underlying technologies.\nLLM2-Querying PDF with AstraDB: In this section, you\u0026rsquo;ll learn about querying PDF documents using AstraDB, exploring techniques and methods for efficiently retrieving information from PDF files.\nLLM3-Blog Generation using LLAMA 2: This section delves into the process of generating blog content using LLAMA 2, an advanced language model tailored for content creation tasks.\nLLM4-LLM Generic App using Pinecone VectorDB: Here, you\u0026rsquo;ll discover how to build a generic application using LLM, leveraging Pinecone VectorDB for managing and querying large-scale vector data efficiently.\nLLM5-Gemini Pro LLM Application: This section focuses on the Gemini Pro LLM Application, showcasing its features, capabilities, and practical applications in various domains.\nLLM6-Invoice Extractor using Gemini Pro Vision: In this section, you\u0026rsquo;ll explore the development and implementation of an invoice extractor using Gemini Pro Vision, a vision-based application powered by LLM technology.\nWhat is CELLSTRAT Hub ? CellStrat Hub is a full-stack AI skilling, development and deployment platform. It enables students and learners to upskill in various domains of AI as well as develop AI projects easily and efficiently. CellStrat Hub\u0026rsquo;s core value is to make AI simpler for everyone. CellStrat Hub includes a wide range of features for development, learning and deployment. Workspace CellStrat Hub Workspace is a simple yet powerful fully-managed development environment for rapid AI development based on JupyterLab. Workspace comes with a whole suite of features like,\nA Dedicated Virtual Machine on the Cloud with Jupyter Lab Persistent Storage for all your Files One-Click Realtime Collaborative Coding (think Google Docs for Code) Visual Debugger to easily debug your swamps of code Code-Snippets Extension to save your frequently used snippets of code and reuse them in any of your projects with the click of a button Seamless Git Integration via both, SSH (recommended) and Personal Access Token A Curated Set of Project Packs with Code Repositories in all major fields of AI like Computer Vision, Natural Language Processing, Reinforcement Learning and Much More.\nExpansion Packs Getting Started with practical machine learning as a beginner is a tedious task with so many projects and options spread all across the internet. Its very easy to get lost in the sea of resources that are out there. To solve this issue, Hub comes with Expansion Packs that are a curated list of projects to get you started in a particular field of AI. If you are a beginner, these packs will help you get hands-on experience and practice. If you are an expert already, these packs will help you jumpstart your project so you can build on top of it.\n","permalink":"http://localhost:1313/blog/aws-dva-certification/","summary":"Introduction I\u0026rsquo;ve developed an LLM(GEN AI) PACK which is deployed on CELLSTRAT Hub. This pack contains a series of projects developed as part of the LangChain initiative, demonstrating various applications and use cases leveraging language models and generative AI. Each project is designed to showcase the capabilities of different technologies and platforms.\nTable Of Contents Here\u0026rsquo;s a brief description for each item in the table of contents:\nLLM1-Basics of LLM: This section covers the fundamentals of LLM (Large Language Models), providing an introduction to its concepts, applications, and underlying technologies.","title":"LLM (LANGCHAIN GEN AI) PACK"},{"content":"Credentials üîó Certificate üîó Credly Badge üé¨ YouTube Video Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with just 2 months of preparation while working full-time as a software engineer. In this article, I‚Äôll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my üé¨ YouTube channel where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by Stephane Maarek which is available on Udemy. While taking this course, I dumped all of the information available in the course into a Notion page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I‚Äôll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada üá®üá¶ for my MS.\n","permalink":"http://localhost:1313/blog/aws-saa-certification/","summary":"Credentials üîó Certificate üîó Credly Badge üé¨ YouTube Video Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with just 2 months of preparation while working full-time as a software engineer. In this article, I‚Äôll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.","title":"I passed the AWS SAA Certification Exam"},{"content":"‚úèÔ∏è Intro If you‚Äôre like me who loves reading books on Kindle, you might have wondered how you could extract your highlights in an organized way and save them as notes. At least I did. You see, I use Notion as my primary note-taking/productivity management app and I already have a database of all the books that I have read so far and also the ones that I am planning to read next.\nAnd since each of these book entries in the Notion database is a page in itself, I thought why not populate them with the highlights that I made in Kindle while reading them. The only problem was Kindle stores all of the highlights in a text file (My Clippings.txt) which as you can see contains a tonne of useless information like the book location, where the highlight was made, and when it was made.\nI needed to find a way to filter out the highlights, group them by the book title and send them to my Notion book database. Not only that, all of this should happen automatically with minimal human effort. So, over the past two weekends, I spent the majority of my time coding and I‚Äôm finally ready with an app that would allow readers to seamlessly transfer all of their highlights to Notion. Let‚Äôs take a look\u0026hellip;\nü§ñ Node Environment You need a stable version of Node JS, installed locally, to run this app. I have tested this on Node versions 16 and 14, and it has worked flawlessly on both of them. So, before proceeding to the next steps, make sure you have a stable version of Node installed. I‚Äôm not going to explain the environment setup in this article because the installation process might differ for different operating systems. You can easily learn that on Google.\n‚öôÔ∏è Setup Follow the steps given below to set up the Kindle to Notion app on your local system.\nCopy my Books Database Template to your Notion dashboard. The app requires some fields (Title, Author, and Book Name) to be present in the database in order for the highlight sync to work properly. So, you can either create your own database having these fields or you can just copy mine using the template I provided above.\nClone the GitHub Repository to your local system and install the dependencies.\ngit clone https://github.com/arkalim/kindle-to-notion.git\rcd kindle-to-notion\rnpm install Rename these files or folders by removing .example extensions as shown below. The original files/folders in my local repo contained data that was either sensitive or specific to my highlights. So, I created empty aliases of them with .example extensions and committed them to GitHub.\n‚Ä£ cache.example ‚û° cache\n‚Ä£ data.example ‚û° data\n‚Ä£ .env.example ‚û° .env\nGet your Notion API key at the Notion Integrations page and create a new internal integration. Integrations allow us to access a portion of our Notion workspace using a secret token called the Notion API key (Internal Integration Token).\nGo to your Notion dashboard. Navigate to the Books database. Click on Share in the top right-hand corner and invite the integration you just created. This will allow the integration to edit the Books database using the Notion API key that we got in the previous step. Copy the link to the Notion Books database and extract the Database Id as shown below. The database id is nothing but all of the gibberish between the last / and the ?. This is required by the app to perform CRUD operations on this database. Original Link: https://www.notion.so/arkalim/346be84507ff482b80fceb4024deadc2?v=e868075eaf5749bc941e617e651295fb\rDatabase Id: 346be84507ff482b80fceb4024deadc2 So, now you have the Notion API key as well as the Database Id. Now, populate these variables in the .env file. Storing this sensitive information in .env ensures that it won‚Äôt get exposed to the rest of the world if you commit your local repo to GitHub as .gitignore has been configured to ignore .env during commits. NOTION_API_KEY=your-notion-api-key\rBOOK_DB_ID=your-book-database-id Connect your Kindle to your computer. Navigate to Kindle ‚û° documents and copy My Clippings.txt. Replace my My Clippings.txt in resources folder with yours. üîÅ Sync Highlights Finally, we are at the end of the setup section. You are now ready to sync your Kindle highlights to Notion. Open a terminal in your local repository and run the following command to watch your highlights teleport!\nnpm start ‚ùóÔ∏èFor Nerds Every highlight made on Kindle is appended at the end of My Clippings.txt. RegEx has been used extensively throughout the application to parse and filter this text file. cache is a folder that contains the local cache to prevent the app from resyncing old highlights. data is a folder that contains the API response logs. .env is a file containing the environment variables like the Notion API key and the Database Id. Book Name is used as the primary key to facilitate upsert operation in the Notion database. Book Name corresponds to the title of the book in My Clippings.txt. So, this field should be left untouched. However, the other fields like Title, Author, Date Started, Date Finished, Status, and Genre could be modified as per your wish. The app maintains a local cache in the file sync.json present in the cache folder. This JSON file is updated at the end of each sync. This is done to prevent the app from resyncing the old highlights. If no new highlights have been made, no sync takes place. In case you wish to sync every book all over again, you need to empty the array present in sync.json and delete all the highlights present in your Notion database before running the sync. Responses from Notion API calls are exported to files with .json extensions in data folder. This was done to mitigate the problem of effectively logging JSON objects in the console (terminal). That‚Äôs all folks! If you made it till here, hats off to you! In this article, we learned how to set up Kindle to Notion app on our local system and use it to sync our Kindle highlights to the Notion Books database. If you want me to write more detailed articles explaining the inner workings of this app, drop a comment below. I write articles regularly so you should consider following me to get more such articles in your feed. Thanks a lot for reading!\n","permalink":"http://localhost:1313/blog/kindle-to-notion/","summary":"‚úèÔ∏è Intro If you‚Äôre like me who loves reading books on Kindle, you might have wondered how you could extract your highlights in an organized way and save them as notes. At least I did. You see, I use Notion as my primary note-taking/productivity management app and I already have a database of all the books that I have read so far and also the ones that I am planning to read next.","title":"Kindle to Notion"},{"content":"Introduction Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\nDataset In this tutorial, we will use the official¬†DLib Dataset¬†which contains¬†6666 images of varying dimensions. Additionally,¬†labels_ibug_300W_train.xml¬†(comes with the dataset) contains the coordinates of¬†68 landmarks for each face. The script below will download the dataset and unzip it in Colab Notebook.\nif not os.path.exists(\u0026#39;/content/ibug_300W_large_face_landmark_dataset\u0026#39;): !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz !tar -xvzf \u0026#39;ibug_300W_large_face_landmark_dataset.tar.gz\u0026#39; !rm -r \u0026#39;ibug_300W_large_face_landmark_dataset.tar.gz\u0026#39; Here is a sample image from the dataset. We can see that the face occupies a very small fraction of the entire image. If we feed the full image to the neural network, it will also process the background (irrelevant information), making it difficult for the model to learn. Therefore, we need to crop the image and feed only the face portion.\nData Preprocessing To prevent the neural network from overfitting the training dataset, we need to randomly transform the dataset. We will apply the following operations to the training and validation dataset:\nSince the face occupies a very small portion of the entire image, crop the image and use only the face for training. Resize the cropped face into a (224x224) image. Randomly change the brightness and saturation of the resized face. Randomly rotate the face after the above three transformations. Convert the image and landmarks into torch tensors and normalize them between [-1, 1]. class Transforms(): def __init__(self): pass def rotate(self, image, landmarks, angle): angle = random.uniform(-angle, +angle) transformation_matrix = torch.tensor([ [+cos(radians(angle)), -sin(radians(angle))], [+sin(radians(angle)), +cos(radians(angle))] ]) image = imutils.rotate(np.array(image), angle) landmarks = landmarks - 0.5 new_landmarks = np.matmul(landmarks, transformation_matrix) new_landmarks = new_landmarks + 0.5 return Image.fromarray(image), new_landmarks def resize(self, image, landmarks, img_size): image = TF.resize(image, img_size) return image, landmarks def color_jitter(self, image, landmarks): color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1) image = color_jitter(image) return image, landmarks def crop_face(self, image, landmarks, crops): left = int(crops[\u0026#39;left\u0026#39;]) top = int(crops[\u0026#39;top\u0026#39;]) width = int(crops[\u0026#39;width\u0026#39;]) height = int(crops[\u0026#39;height\u0026#39;]) image = TF.crop(image, top, left, height, width) img_shape = np.array(image).shape landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]]) landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]]) return image, landmarks def __call__(self, image, landmarks, crops): image = Image.fromarray(image) image, landmarks = self.crop_face(image, landmarks, crops) image, landmarks = self.resize(image, landmarks, (224, 224)) image, landmarks = self.color_jitter(image, landmarks) image, landmarks = self.rotate(image, landmarks, angle=10) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) return image, landmarks Dataset Class Now that we have our transformations ready, let‚Äôs write our dataset class. The¬†labels_ibug_300W_train.xml¬†contains the image path, landmarks and coordinates for the bounding box (for cropping the face).¬†We will store these values in lists to access them easily during training.¬†In this tutorial, the neural network will be trained on grayscale images.\nclass FaceLandmarksDataset(Dataset): def __init__(self, transform=None): tree = ET.parse(\u0026#39;ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml\u0026#39;) root = tree.getroot() self.image_filenames = [] self.landmarks = [] self.crops = [] self.transform = transform self.root_dir = \u0026#39;ibug_300W_large_face_landmark_dataset\u0026#39; for filename in root[2]: self.image_filenames.append(os.path.join(self.root_dir, filename.attrib[\u0026#39;file\u0026#39;])) self.crops.append(filename[0].attrib) landmark = [] for num in range(68): x_coordinate = int(filename[0][num].attrib[\u0026#39;x\u0026#39;]) y_coordinate = int(filename[0][num].attrib[\u0026#39;y\u0026#39;]) landmark.append([x_coordinate, y_coordinate]) self.landmarks.append(landmark) self.landmarks = np.array(self.landmarks).astype(\u0026#39;float32\u0026#39;) assert len(self.image_filenames) == len(self.landmarks) def __len__(self): return len(self.image_filenames) def __getitem__(self, index): image = cv2.imread(self.image_filenames[index], 0) landmarks = self.landmarks[index] if self.transform: image, landmarks = self.transform(image, landmarks, self.crops[index]) landmarks = landmarks - 0.5 return image, landmarks dataset = FaceLandmarksDataset(Transforms()) Note:¬†landmarks = landmarks - 0.5¬†is done to zero-centre the landmarks as zero-centred outputs are easier for the neural network to learn.\nThe output of the dataset after preprocessing will look something like this (landmarks have been plotted on the image).\nNeural Network We will use the ResNet18 as the basic framework. We need to modify the first and last layers to suit our purpose. In the first layer, we will make the input channel count as 1 for the neural network to accept grayscale images. Similarly, in the final layer, the output channel count should equal¬†68 * 2 = 136¬†for the model to predict the (x, y) coordinates of the 68 landmarks for each face.\nclass Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name=\u0026#39;resnet18\u0026#39; self.model=models.resnet18() self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features, num_classes) def forward(self, x): x=self.model(x) return x Training the Neural Network We will use the Mean Squared Error between the predicted landmarks and the true landmarks as the loss function. Keep in mind that the learning rate should be kept low to avoid exploding gradients. The network weights will be saved whenever the validation loss reaches a new minimum value. Train for at least 20 epochs to get the best performance.\nnetwork = Network() network.cuda() criterion = nn.MSELoss() optimizer = optim.Adam(network.parameters(), lr=0.0001) loss_min = np.inf num_epochs = 10 start_time = time.time() for epoch in range(1,num_epochs+1): loss_train = 0 loss_valid = 0 running_loss = 0 network.train() for step in range(1,len(train_loader)+1): images, landmarks = next(iter(train_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # clear all the gradients before calculating them optimizer.zero_grad() # find the loss for the current step loss_train_step = criterion(predictions, landmarks) # calculate the gradients loss_train_step.backward() # update the parameters optimizer.step() loss_train += loss_train_step.item() running_loss = loss_train/step print_overwrite(step, len(train_loader), running_loss, \u0026#39;train\u0026#39;) network.eval() with torch.no_grad(): for step in range(1,len(valid_loader)+1): images, landmarks = next(iter(valid_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # find the loss for the current step loss_valid_step = criterion(predictions, landmarks) loss_valid += loss_valid_step.item() running_loss = loss_valid/step print_overwrite(step, len(valid_loader), running_loss, \u0026#39;valid\u0026#39;) loss_train /= len(train_loader) loss_valid /= len(valid_loader) print(\u0026#39;\\n--------------------------------------------------\u0026#39;) print(\u0026#39;Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}\u0026#39;.format(epoch, loss_train, loss_valid)) print(\u0026#39;--------------------------------------------------\u0026#39;) if loss_valid \u0026lt; loss_min: loss_min = loss_valid torch.save(network.state_dict(), \u0026#39;/content/face_landmarks.pth\u0026#39;) print(\u0026#34;\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\u0026#34;.format(loss_min, epoch, num_epochs)) print(\u0026#39;Model Saved\\n\u0026#39;) print(\u0026#39;Training Complete\u0026#39;) print(\u0026#34;Total Elapsed Time : {} s\u0026#34;.format(time.time()-start_time)) Predict on Unseen Data Use the code snippet below to predict landmarks in unseen images.\nimport time import cv2 import os import numpy as np import matplotlib.pyplot as plt from PIL import Image import imutils import torch import torch.nn as nn from torchvision import models import torchvision.transforms.functional as TF ####################################################################### image_path = \u0026#39;pic.jpg\u0026#39; weights_path = \u0026#39;face_landmarks.pth\u0026#39; frontal_face_cascade_path = \u0026#39;haarcascade_frontalface_default.xml\u0026#39; ####################################################################### class Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name=\u0026#39;resnet18\u0026#39; self.model=models.resnet18(pretrained=False) self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features,num_classes) def forward(self, x): x=self.model(x) return x ####################################################################### face_cascade = cv2.CascadeClassifier(frontal_face_cascade_path) best_network = Network() best_network.load_state_dict(torch.load(weights_path, map_location=torch.device(\u0026#39;cpu\u0026#39;))) best_network.eval() image = cv2.imread(image_path) grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) height, width,_ = image.shape faces = face_cascade.detectMultiScale(grayscale_image, 1.1, 4) all_landmarks = [] for (x, y, w, h) in faces: image = grayscale_image[y:y+h, x:x+w] image = TF.resize(Image.fromarray(image), size=(224, 224)) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) with torch.no_grad(): landmarks = best_network(image.unsqueeze(0)) landmarks = (landmarks.view(68,2).detach().numpy() + 0.5) * np.array([[w, h]]) + np.array([[x, y]]) all_landmarks.append(landmarks) plt.figure() plt.imshow(display_image) for landmarks in all_landmarks: plt.scatter(landmarks[:,0], landmarks[:,1], c = \u0026#39;c\u0026#39;, s = 5) plt.show() ‚ö†Ô∏è The above code snippet will not work in Colab Notebook as some functionality of the OpenCV is not supported in Colab yet. To run the above cell, use your local machine.\nOpenCV Harr Cascade Classifier is used to detect faces in an image. Object detection using Haar Cascades is a machine learning-based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, pedestrians, and many more. In our case, we will be using the face classifier for which you need to download the pre-trained classifier XML file and save it to your working directory.\nDetected faces in the input image are then cropped, resized to (224, 224) and fed to our trained neural network to predict landmarks in them.\nThe predicted landmarks in the cropped faces are then overlayed on top of the original image. The result is the image shown below. Pretty impressive, right!\nSimilarly, landmarks detection on multiple faces:\nHere, you can see that the OpenCV Harr Cascade Classifier has detected multiple faces including a false positive (a fist is predicted as a face). So, the network has plotted some landmarks on that.\nThat‚Äôs all folks! If you made it till here, hats off to you! You just trained your very own neural network to detect face landmarks in any image. Try predicting face landmarks on your webcam feed!!\nColab Notebook The complete code can be found in the interactive Colab Notebook.\n","permalink":"http://localhost:1313/blog/face-landmarks-detection/","summary":"Introduction Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\nDataset In this tutorial, we will use the official¬†DLib Dataset¬†which contains¬†6666 images of varying dimensions. Additionally,¬†labels_ibug_300W_train.xml¬†(comes with the dataset) contains the coordinates of¬†68 landmarks for each face.","title":"Face Landmarks Detection using CNN"},{"content":"Introduction In this article, we will learn how PCA can be used to¬†compress a real-life dataset. We will be working with¬†Labelled Faces in the Wild (LFW),¬†a large scale dataset consisting of¬†13233¬†human-face grayscale images, each having a dimension of¬†64x64. It means that the data for each face is 4096 dimensional (there are 64x64 = 4096 unique values to be stored for each face). We will reduce this dimension requirement, using PCA, to just a few hundred dimensions!\nPrincipal Component Analysis (PCA) Principal component analysis (PCA) is a technique for reducing the dimensionality of datasets, exploiting the fact that the images in these datasets have something in common. For instance, in a dataset consisting of face photographs, each photograph will have facial features like eyes, nose, mouth. Instead of encoding this information pixel by pixel, we could make a template of each type of these features and then just combine these templates to generate any face in the dataset. In this approach, each template will still be 64x64 = 4096 dimensional, but since we will be reusing these templates (basis functions) to generate each face in the dataset, the number of templates required will be small. PCA does exactly this. Let‚Äôs see how!\nDataset Let‚Äôs visualize some images from the dataset. You can see that each image has a complete face, and the facial features like eyes, nose, and lips are clearly visible in each image. Now that we have our dataset ready, let‚Äôs compress it.\nCompression PCA is a 4 step process.¬†Starting with a dataset containing¬†n¬†dimensions (requiring¬†n-axes to be represented):\nStep¬†1: Find a new set of basis functions (naxes) where some axes contribute to most of the variance in the dataset while others contribute very little. Step 2: Arrange these axes in the decreasing order of variance contribution. Step 3: Now, pick the top¬†k¬†axes to be used and drop the remaining¬†n-k¬†axes. Step 4: Now, project the dataset onto these¬†k¬†axes. These steps are well explained in my previous article. After these 4 steps, the dataset will be compressed from¬†n-dimensions to just¬†k-dimensions (k\u0026lt;n).\nStep 1 Finding a new set of basis functions (n-axes), where some axes contribute to most of the variance in the dataset while others contribute very little, is analogous to finding the templates that we will combine later to generate faces in the dataset. A total of 4096 templates, each 4096 dimensional, will be generated. Each face in the dataset can be represented as a linear combination of these templates.\nPlease note that the scalar constants (k1, k2, ‚Ä¶, kn) will be unique for each face.\nStep 2 Now, some of these templates contribute significantly to facial reconstruction while others contribute very little. This level of contribution can be quantified as the percentage of variance that each template contributes to the dataset. So, in this step, we will arrange these templates in the decreasing order of variance contribution (most significant‚Ä¶least significant).\nStep 3 Now, we will keep the top¬†k¬†templates and drop the remaining. But, how many templates shall we keep? If we keep more templates, our reconstructed images will closely resemble the original images but we will need more storage to store the compressed data. If we keep too few templates, our reconstructed images will look very different from the original images.\nThe best solution is to fix the percentage of variance that we want to retain in the compressed dataset and use this to determine the value of¬†k¬†(number of templates to keep). If we do the math, we find that to retain¬†99%¬†of the variance, we need only the top¬†577¬†templates. We will save these values in an array and drop the remaining templates.\nLet‚Äôs visualize some of these selected templates.\nPlease note that each of these templates looks somewhat like a face. These are called as Eigenfaces.\nStep 4 Now, we will construct a projection matrix to project the images from the original 4096 dimensions to just 577 dimensions. The projection matrix will have a shape¬†(4096, 577), where the templates will be the columns of the matrix.\nBefore we go ahead and compress the images, let‚Äôs take a moment to understand what we really mean by compression. Recall that the faces can be generated by a linear combination of the selected templates. As each face is unique, every face in the dataset will require a different set of constants (k1, k2, ‚Ä¶, kn) for the linear combination.\nLet‚Äôs start with an image from the dataset and compute the constants (k1, k2, ‚Ä¶, kn), where n = 577. These constants along with the selected 577 templates can be plugged in the equation above to reconstruct the face. This means that we only need to compute and save these 577 constants for each image. Instead of doing this image by image, we can use matrices to compute these constants for each image in the dataset at the same time.\nRecall that there are 13233 images in the dataset. The matrix compressed_images contains the 577 constants for each image in the dataset. We can now say that we have compressed our images from 4096 dimensions to just 577 dimensions while retaining 99% of the information.\nCompression Ratio Let‚Äôs calculate how much we have compressed the dataset. Recall that there are 13233 images in the dataset and each image is 64x64 dimensional. So, the total number of unique values required to store the original dataset is13233 x 64 x 64 =¬†54,202,368 unique values.\nAfter compression, we store 577 constants for each image. So, the total number of unique values required to store the compressed dataset is13233 x 577 = 7,635,441 unique values. But, we also need to store the templates to reconstruct the images later. Therefore, we also need to store577 x 64 x 64 = 2,363,392 unique values for the templates. Therefore, the total number of unique values required to store the compressed dataset is7,635,441 + 2,363,392 =¬†9,998,883 unique values.\nWe can calculate the percentage compression as:\nReconstruct the Images The compressed images are just arrays of length 577 and can‚Äôt be visualized as such. We need to reconstruct it back to 4096 dimensions to view it as an array of shape (64x64). Recall that each template has a dimension of 64x64 and that each constant is a scalar value. We can use the equation below to reconstruct any face in the dataset.\nAgain, instead of doing this image by image, we can use matrices to reconstruct the whole dataset at once, with of course a loss of 1% variance.\nLet‚Äôs look at some reconstructed faces.\nWe can see that the reconstructed images have captured most of the relevant information about the faces and the unnecessary details have been ignored. This is an added advantage of data compression, it allows us to filter unnecessary details (and even noise) present in the data.\nThat‚Äôs all folks! If you made it till here, hats off to you! In this article, we learnt how PCA can be used to compress¬†Labelled Faces in the Wild (LFW),¬†a large scale dataset consisting of 13233 human-face images, each having a dimension of¬†64x64. We compressed this dataset by over 80% while retaining 99% of the information.\nColab Notebook View my Colab Notebook for a well commented code!\n","permalink":"http://localhost:1313/blog/face-dataset-compression/","summary":"Introduction In this article, we will learn how PCA can be used to¬†compress a real-life dataset. We will be working with¬†Labelled Faces in the Wild (LFW),¬†a large scale dataset consisting of¬†13233¬†human-face grayscale images, each having a dimension of¬†64x64. It means that the data for each face is 4096 dimensional (there are 64x64 = 4096 unique values to be stored for each face). We will reduce this dimension requirement, using PCA, to just a few hundred dimensions!","title":"Face Dataset Compression using PCA"},{"content":"üîó GitHub Description I had created this hostel website \u0026ldquo;BMS INN\u0026rdquo; as my college web dev project in my 3rd semester solely on my own. It took me a month to complete this project. Being a hostelite, this project is more like an emotion to me. This website is built in HTML, CSS and JavaScript.\nMotivation Being a hostelite myself, I recognized the need for a hostel website to optimize our college experience. My website aims to simplify hostel life by providing a centralized platform for communication, information sharing, and accessing essential resources.\nImages Architecture Overview The main heading of the website is \u0026ldquo;BMS Hostel,\u0026rdquo; with five main categories or sections: \u0026ldquo;Admission,\u0026rdquo; \u0026ldquo;Facilities,\u0026rdquo; \u0026ldquo;Events,\u0026rdquo; \u0026ldquo;Testimonial,\u0026rdquo; and \u0026ldquo;Contact\u0026rdquo; along with Login and SignUp options.\nUnder the \u0026ldquo;Admission\u0026rdquo; section, there are three sub-sections: \u0026ldquo;Admission Process,\u0026rdquo; \u0026ldquo;Allotment of Room,\u0026rdquo; and \u0026ldquo;Vacating the Room.\u0026rdquo; The \u0026ldquo;Facilities\u0026rdquo; section is also divided into three sub-sections: \u0026ldquo;Accommodation,\u0026rdquo; \u0026ldquo;Hostel Mess,\u0026rdquo; and \u0026ldquo;Hospital.\u0026rdquo; Within the \u0026ldquo;Events\u0026rdquo; section, users can explore \u0026ldquo;Rafaga,\u0026rdquo; \u0026ldquo;Food Fest,\u0026rdquo; and \u0026ldquo;Hostel Days.\u0026rdquo; Both the \u0026ldquo;Testimonial\u0026rdquo; and \u0026ldquo;Contact\u0026quot;sections remain undivided. Login and SignUp is essential to view the contents of Events and Facilities in depth. ","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"üîó GitHub Description I had created this hostel website \u0026ldquo;BMS INN\u0026rdquo; as my college web dev project in my 3rd semester solely on my own. It took me a month to complete this project. Being a hostelite, this project is more like an emotion to me. This website is built in HTML, CSS and JavaScript.\nMotivation Being a hostelite myself, I recognized the need for a hostel website to optimize our college experience.","title":"BMS INN"},{"content":"üîó GitHub üîó LinkedIn Post Description ATS Resume Expert ATS Resume Expert is a Streamlit web application designed to assist in analyzing resumes using Google\u0026rsquo;s Generative AI, specifically the Gemini Pro model. This tool is especially useful for HR professionals and job seekers, as it helps in evaluating the alignment of a resume with a specific job description.\nFeatures: Resume Upload: Users can upload their resume in PDF format. Job Description Analysis: The application allows users to input a job description. Resume Evaluation: It provides an evaluation of the uploaded resume against the provided job description. Matching Percentage: The tool calculates a percentage match, indicating how well the resume aligns with the job requirements. Strengths and Weaknesses Analysis: Offers insights into the strengths and weaknesses of the application in relation to the specified job requirements. Images How It Works: Upload Resume: Users upload their resume in PDF format. Input Job Description: Users enter a job description in the provided text area. Analysis: Upon clicking the analysis button, the app uses Google\u0026rsquo;s Gemini Pro model to analyze the resume against the job description. Display Results: The app displays the analysis results, including the matching percentage and detailed feedback. Technical Details: Streamlit: This application is built using Streamlit, a powerful framework for building data applications. Google Generative AI: It uses Google\u0026rsquo;s Generative AI (Gemini Pro model) for analyzing resumes. PDF Processing: The app converts PDF resumes to images for processing. Environment Variables: API keys and sensitive data are stored in environment variables for security. ","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"üîó GitHub üîó LinkedIn Post Description ATS Resume Expert ATS Resume Expert is a Streamlit web application designed to assist in analyzing resumes using Google\u0026rsquo;s Generative AI, specifically the Gemini Pro model. This tool is especially useful for HR professionals and job seekers, as it helps in evaluating the alignment of a resume with a specific job description.\nFeatures: Resume Upload: Users can upload their resume in PDF format. Job Description Analysis: The application allows users to input a job description.","title":"ATS Resume Expert"},{"content":"üîó View App üîó GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"üîó View App üîó GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.","title":"Todo List App"},{"content":"üîó GitHub üîó LinkedIn Post Description In this project, I created the \u0026ldquo;Pika-Food Ordering Chatbot\u0026rdquo; utilizing Python, FastAPI, MySQL, and DialogFlow to streamline the food ordering process. The chatbot offers users an intuitive platform for placing orders, featuring effortless ordering, real-time interaction, and seamless database integration. Leveraging FastAPI, I ensured smooth communication between the user interface and the backend, enabling dynamic updates and interactions for a responsive ordering experience.\nAdditionally, I focused on enhancing user satisfaction by incorporating advanced features such as order completion brilliance and NLP-powered conversations. The chatbot calculates totals, generates unique order IDs, and provides detailed summaries, ensuring a seamless order completion process. Moreover, NLP technology adds a personalized touch to interactions, making the chatbot easy and enjoyable to use. Join the chatbot revolution and simplify your food ordering experience with the \u0026ldquo;Pika-Food Ordering Chatbot\u0026rdquo; today! üçîü§ñüåü\nImages ","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"üîó GitHub üîó LinkedIn Post Description In this project, I created the \u0026ldquo;Pika-Food Ordering Chatbot\u0026rdquo; utilizing Python, FastAPI, MySQL, and DialogFlow to streamline the food ordering process. The chatbot offers users an intuitive platform for placing orders, featuring effortless ordering, real-time interaction, and seamless database integration. Leveraging FastAPI, I ensured smooth communication between the user interface and the backend, enabling dynamic updates and interactions for a responsive ordering experience.\nAdditionally, I focused on enhancing user satisfaction by incorporating advanced features such as order completion brilliance and NLP-powered conversations.","title":"Pika Food Ordering Chatbot (END-END NLP PROJECT)"},{"content":"üîó GitHub üîó LinkedIn Post Description \u0026ldquo;The Award-Winning Science \u0026amp; Tech Chatbot Triumphs at \u0026lsquo;Hackaphasia\u0026rsquo; AI Hackathon!\u0026rdquo;\nThe aim of this project was to craft a cutting-edge Science and Technology chatbot using Python, NLP techniques, spaCy, Gradio, and Hugging Face transformers alongside my talented teammates Dhawan and Vasu, securing the 3rd position at \u0026lsquo;Hackaphasia,\u0026rsquo; the 24-hour AI hackathon organized by IEEE Computer Society during the Phaseshift event at our college.\nOur bot showcased advanced features such as semantic understanding, information extraction, interactivity, summarization, user query learning, and scalability. We meticulously engineered it to extract embeddings from text using spaCy and a custom model, with Hugging Face transformers enhancing our summarization capabilities.\nImages:\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"üîó GitHub üîó LinkedIn Post Description \u0026ldquo;The Award-Winning Science \u0026amp; Tech Chatbot Triumphs at \u0026lsquo;Hackaphasia\u0026rsquo; AI Hackathon!\u0026rdquo;\nThe aim of this project was to craft a cutting-edge Science and Technology chatbot using Python, NLP techniques, spaCy, Gradio, and Hugging Face transformers alongside my talented teammates Dhawan and Vasu, securing the 3rd position at \u0026lsquo;Hackaphasia,\u0026rsquo; the 24-hour AI hackathon organized by IEEE Computer Society during the Phaseshift event at our college.","title":"NLP Powered Chatbot to Explore Science and Technologies"},{"content":"üîó GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"üîó GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.","title":"OpenQuad"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter.","title":"SEBART-Pro"},{"content":"Description Strategically optimized the product\u0026rsquo;s infrastructure on AWS, resulting in a remarkable cost reduction of over 50% while fortifying network security. Improved the product\u0026rsquo;s availability and fault tolerance through dynamic horizontal scaling on AWS, ensuring uninterrupted service and enhancing user experience. Automated integration testing using PyTest, effectively saving over 4 hours of manual testing time in each sprint. Asynchronously decoupled individual micro-services and introduced dead letter queues (DLQ) to ensure reliable and uninterrupted operation of the product‚Äôs pipeline. Developed Python scripts to streamline product installation on customer sites, significantly reducing onboarding time by over 200%. Performed a comprehensive revamp of the backend codebase, improving readability, fixing bugs, and enhancing performance by identifying and resolving bottlenecks. Actively engaged with potential customers as the lead developer, providing technical guidance and support to drive customer success which increased product adoption by over 20%. Led and mentored a team of 3 junior interns in successfully designing, developing, and seamlessly integrating a sophisticated AI pipeline with the product. Building a cloud-native version of the product on Kubernetes with GitOps principles, to facilitate easy deployment and management of multiple instances of the product on the cloud. ","permalink":"http://localhost:1313/experience/16bit/","summary":"Description Strategically optimized the product\u0026rsquo;s infrastructure on AWS, resulting in a remarkable cost reduction of over 50% while fortifying network security. Improved the product\u0026rsquo;s availability and fault tolerance through dynamic horizontal scaling on AWS, ensuring uninterrupted service and enhancing user experience. Automated integration testing using PyTest, effectively saving over 4 hours of manual testing time in each sprint. Asynchronously decoupled individual micro-services and introduced dead letter queues (DLQ) to ensure reliable and uninterrupted operation of the product‚Äôs pipeline.","title":"DevOps Intern"},{"content":"Description Developed an event-driven serverless integration framework using AWS Lambda and EventBridge, facilitating seamless synchronization of customer data between Salesforce and BuyerAssist. Designed and implemented a configuration-driven framework to enhance the pattern-matching capability of AWS EventBridge, preventing over 1000 false invocations of AWS Lambda functions every day. Implemented a system to track asynchronous data transfer jobs through AWS AppFlow, which reduced the issue tracking time to under 5 mins. Developed a Salesforce app using SFDX to provide clients a customized experience within their Salesforce dashboard. Developed a Slack bot that sent interactive daily notifications to customers, enabling them to take direct actions from Slack. This streamlined operations and boosted sales by over 50%. ","permalink":"http://localhost:1313/experience/buyerassist/","summary":"Description Developed an event-driven serverless integration framework using AWS Lambda and EventBridge, facilitating seamless synchronization of customer data between Salesforce and BuyerAssist. Designed and implemented a configuration-driven framework to enhance the pattern-matching capability of AWS EventBridge, preventing over 1000 false invocations of AWS Lambda functions every day. Implemented a system to track asynchronous data transfer jobs through AWS AppFlow, which reduced the issue tracking time to under 5 mins. Developed a Salesforce app using SFDX to provide clients a customized experience within their Salesforce dashboard.","title":"Backend Engineer"},{"content":"üîó GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans Addressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods Developed a custom loss function combining CGAN loss and Dice Loss for improved image segmentation Implemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans Leveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training Utilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition ","permalink":"http://localhost:1313/experience/tumunich/","summary":"üîó GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans Addressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods Developed a custom loss function combining CGAN loss and Dice Loss for improved image segmentation Implemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans Leveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training Utilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition ","title":"Remote Research Intern"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","title":"Software Intern"},{"content":"üîó GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"üîó GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","title":"Computer Vision Intern"}]